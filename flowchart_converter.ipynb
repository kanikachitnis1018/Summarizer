{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGwQztV6x3ZirzHIVPAaWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanikachitnis1018/Summarizer/blob/main/flowchart_converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z4Su8FkD0SJ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import re\n"
      ],
      "metadata": {
        "id": "o81dBWsKEYk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_be_split = \"\"\" Kasparov was a fiercely aggressive chess player who thrived on energy and confidence. My father wrote a book called Mortal Games about Garry, and during the years surrounding the 1990 Kasparov-Karpov match, we both spent quite a lot of time with him.\n",
        "\n",
        "At one point, after Kasparov had lost a big game and was feeling dark and fragile, my father asked Garry how he would handle his lack of confidence in the next game. Garry responded that he would try to play the chess moves that he would have played if he were feeling confident. He would pretend to feel confident, and hopefully trigger the state.\n",
        "\n",
        "Kasparov was an intimidator over the board. Everyone in the chess world was afraid of Garry and he fed on that reality. If Garry bristled at the chessboard, opponents would wither. So if Garry was feeling bad, but puffed up his chest, made aggressive moves, and appeared to be the manifestation of Confidence itself, then opponents would become unsettled. Step by step, Garry would feed off his own chess moves, off the created position, and off his opponent's building fear, until soon enough the confidence would become real and Garry would be in flowâ€¦\n",
        "\n",
        "He was not being artificial. Garry was triggering his zone by playing Kasparov chess \"\"\"\n",
        "\n",
        "paragraphs = text_to_be_split.split(\"\\n\\n\")\n",
        "\n",
        "paragraphs"
      ],
      "metadata": {
        "id": "nQGIDQujEe6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_paras = []\n",
        "\n",
        "for para in paragraphs:\n",
        "    para = para.strip()\n",
        "\n",
        "    para = re.sub(r'\\s+', ' ', para)\n",
        "\n",
        "    para = re.sub(r'[^a-zA-Z0-9.,;:!?()\\'\" -]', '', para)\n",
        "\n",
        "    cleaned_paras.append(para)\n",
        "\n",
        "for i, para in enumerate(cleaned_paras, 1):\n",
        "    print(f\"Paragraph {i}: {para}\\n\")"
      ],
      "metadata": {
        "id": "8L3lF6wwEn5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from itertools import chain\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "sentences = list(chain.from_iterable(map(sent_tokenize, cleaned_paras)))"
      ],
      "metadata": {
        "id": "39VmDmn6PA5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "encoded = tokenizer(    # converts sentences to tokens\n",
        "    sentences,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "with torch.no_grad():   # converts tokens to token embeddings\n",
        "    outputs = model(**encoded)\n",
        "\n",
        "embeddings = outputs.last_hidden_state    # converts token embeddings to sentence embeddings\n",
        "sentence_embeddings = embeddings.mean(dim=1)\n",
        "\n",
        "sentences_per_para = list(map(sent_tokenize, cleaned_paras))\n",
        "para_lengths = list(map(len, sentences_per_para))\n",
        "\n",
        "sentence_groups = torch.split(sentence_embeddings, para_lengths)\n"
      ],
      "metadata": {
        "id": "79K5HmUYWnuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from operator import itemgetter\n",
        "\n",
        "def summarize_all_paragraphs(sentence_groups, sentences_per_para, k=2):\n",
        "    def summarize_one(group_sents):\n",
        "        group, para_sents = group_sents\n",
        "        if group.size(0) == 0:\n",
        "            return \"\"\n",
        "\n",
        "        para_embedding = group.mean(dim=0, keepdim=True)\n",
        "        scores = F.cosine_similarity(group, para_embedding)\n",
        "        k_safe = min(k, group.size(0))\n",
        "\n",
        "        top_idx = scores.topk(k_safe).indices\n",
        "        sorted_idx, _ = torch.sort(top_idx)\n",
        "\n",
        "        selected = itemgetter(*sorted_idx.tolist())(para_sents)\n",
        "\n",
        "        if isinstance(selected, tuple):\n",
        "            return \" \".join(selected)\n",
        "        else:\n",
        "            return selected\n",
        "\n",
        "    return list(map(summarize_one, zip(sentence_groups, sentences_per_para)))\n"
      ],
      "metadata": {
        "id": "dCSE1rUckpHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_summaries = summarize_all_paragraphs(sentence_groups, sentences_per_para, k=4)\n",
        "\n",
        "local_summaries"
      ],
      "metadata": {
        "id": "NJsSoDNEo1-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "inputs = bart_tokenizer(\n",
        "    local_summaries,\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=1024\n",
        ")\n",
        "\n",
        "summary_ids = bart_model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    num_beams=4,\n",
        "    max_length=80,\n",
        "    min_length=20,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "final_summaries = bart_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "\n",
        "print(final_summaries)\n"
      ],
      "metadata": {
        "id": "rjn_7GiT6ZvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "id": "oRPXkTc2Y-6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "\n",
        "kw_extractor = yake.KeywordExtractor(lan=\"en\", n=2, top=1)\n",
        "\n",
        "titles = list(map(lambda summary: kw_extractor.extract_keywords(summary)[0][0], final_summaries))\n",
        "\n",
        "print(titles)\n"
      ],
      "metadata": {
        "id": "lUQDhtyM_UWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4sAER8dBRnD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}